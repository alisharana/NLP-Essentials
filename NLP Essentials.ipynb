{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb2484c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\alisha rana\\anaconda3\\lib\\site-packages (3.6.5)\n",
      "Requirement already satisfied: click in c:\\users\\alisha rana\\anaconda3\\lib\\site-packages (from nltk) (8.0.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\alisha rana\\anaconda3\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\alisha rana\\anaconda3\\lib\\site-packages (from nltk) (2021.8.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\alisha rana\\anaconda3\\lib\\site-packages (from nltk) (4.62.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\alisha rana\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3936794f",
   "metadata": {},
   "source": [
    "! pip install regex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c020143e",
   "metadata": {},
   "source": [
    "A regular expression (or RE) specifies a set of strings that matches it\n",
    "The Natural Language Toolkit (NLTK) is a platform used for building Python programs \n",
    "that work with human language data for applying in statistical natural language processing (NLP).\n",
    "It contains text processing libraries for tokenization,\n",
    "parsing, classification, stemming, tagging and semantic reasoning.\n",
    "The process of converting data to something a computer can understand is referred to as pre-processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff3ade94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fbd482",
   "metadata": {},
   "source": [
    "The process of converting data to something a computer can understand is referred to as pre-processing. One of the major forms of pre-processing is to filter out useless data. In natural language processing, useless words (data), are referred to as stop words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e97ccc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9fb36d",
   "metadata": {},
   "source": [
    "Stemming is the process of reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words known as a lemma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4f22537",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f666766c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('C:/Users/ALISHA RANA/Downloads/nlpdataset/Test.csv' , encoding='cp437')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "487178bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=dataset.iloc[0:5000,1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db02590e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>∩╗┐text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I always wrote this series off as being a comp...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1st watched 12/7/2002 - 3 out of 10(Dir-Steve ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This movie was so poorly written and directed ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The most interesting thing about Miryang (Secr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>when i first read about \"berlin am meer\" i did...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             ∩╗┐text  label\n",
       "0  I always wrote this series off as being a comp...      0\n",
       "1  1st watched 12/7/2002 - 3 out of 10(Dir-Steve ...      0\n",
       "2  This movie was so poorly written and directed ...      0\n",
       "3  The most interesting thing about Miryang (Secr...      1\n",
       "4  when i first read about \"berlin am meer\" i did...      0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e82da41c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\ALISHA\n",
      "[nltk_data]     RANA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "36ebb2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=[]\n",
    "for i in range(0,5000):\n",
    "    review = re.sub('[^a-zA-Z]',' ',dataset['∩╗┐text'][i])\n",
    "    review=review.lower()\n",
    "    review=review.split()\n",
    "    ps=PorterStemmer()\n",
    "    review=[ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "    review=' '.join(review)\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a74c84d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer    \n",
    "cv=CountVectorizer(max_features=2500)\n",
    "x=cv.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b6cad4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.20, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "17d29439",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier=RandomForestClassifier(n_estimators=10)\n",
    "classifier.fit(X_train,y_train)\n",
    "pred=classifier.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0311ccf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix\n",
      "[[391  98]\n",
      " [173 338]]\n",
      "Accuracy score: 0.729\n",
      "Precision score: 0.7752293577981652\n",
      "Recall score: 0.6614481409001957\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score,precision_score,recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm=confusion_matrix(y_test,pred)\n",
    "print(\"confusion matrix\")\n",
    "print(cm)\n",
    "print('Accuracy score: {}'.format(accuracy_score(y_test, pred)))\n",
    "print('Precision score: {}'.format(precision_score(y_test, pred)))\n",
    "print('Recall score: {}'.format(recall_score(y_test, pred)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
